{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Analysis and Machine Learning on Crime Data\n",
    "\n",
    "This notebook performs data analysis and machine learning on crime data. It includes data import, visualization, preprocessing, and model training."
   ],
   "metadata": {
    "id": "J2tNWh6XRcKy"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install Required Libraries"
  },
  {
   "cell_type": "code",
   "source": [
    "is_upgrade_pip: bool = False\n",
    "\n",
    "def upgrade_pip():\n",
    "    !pip install --upgrade pip > /dev/null\n",
    "    global is_upgrade_pip\n",
    "    is_upgrade_pip = True\n",
    "\n",
    "try :\n",
    "    import pandas as pd\n",
    "except Exception as e:\n",
    "    if not is_upgrade_pip:\n",
    "        upgrade_pip()\n",
    "    !pip install pandas > /dev/null\n",
    "try:\n",
    "    import plotly\n",
    "except Exception as e:\n",
    "    if not is_upgrade_pip:\n",
    "        upgrade_pip()\n",
    "    !pip install plotly > /dev/null\n",
    "try:\n",
    "    import category_encoders\n",
    "except Exception as e:\n",
    "    if not is_upgrade_pip:\n",
    "        upgrade_pip()\n",
    "    !pip install category_encoders > /dev/null\n",
    "try:\n",
    "    import sklearn\n",
    "except Exception as e:\n",
    "    if not is_upgrade_pip:\n",
    "        upgrade_pip()\n",
    "    !pip install sklearn > /dev/null\n",
    "try:\n",
    "    import matplotlib\n",
    "except Exception as e:\n",
    "    if not is_upgrade_pip:\n",
    "        upgrade_pip()\n",
    "    !pip install matplotlib > /dev/null"
   ],
   "metadata": {
    "id": "ot_AtKRZJ4vQ"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import plotly.express as px\n",
    "from category_encoders import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from collections import Counter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define Functions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def import_csv(file_path):\n",
    "    \"\"\"Import a CSV file and return a pandas DataFrame.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(\"Error: File is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(\"Error: Parsing issue in the file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "def create_file_path(*path_segments):\n",
    "    \"\"\"Create a file path from path segments.\"\"\"\n",
    "    return os.path.join(*path_segments)\n",
    "\n",
    "def graphpoint(datas, x, y, color):\n",
    "    \"\"\"Display a scatter plot.\"\"\"\n",
    "    fig = px.scatter(datas, x=x, y=y, color=color)\n",
    "    fig.show()\n",
    "\n",
    "def graphbox(datas):\n",
    "    \"\"\"Display a box plot.\"\"\"\n",
    "    datas.plot.box()\n",
    "\n",
    "def graphhistogram(datas, x, color):\n",
    "    \"\"\"Return a histogram plot.\"\"\"\n",
    "    return px.histogram(datas, x=x, color=color)\n",
    "\n",
    "def datasprint(datas, describe, head):\n",
    "    \"\"\"Print data description and head.\"\"\"\n",
    "    if describe:\n",
    "        print(datas.describe())\n",
    "    if head:\n",
    "        print(datas.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load CSV Files"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = './DataSet'\n",
    "file_path = create_file_path(directory, \"test.csv\")\n",
    "file_path2 = create_file_path(directory, \"train.csv\")\n",
    "\n",
    "df_test = import_csv(file_path)\n",
    "df_train = import_csv(file_path2)\n",
    "\n",
    "if df_test is None or df_train is None:\n",
    "    raise Exception(\"One of the dataframes is empty\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Display Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"{file_path} :\")\n",
    "datasprint(df_test, describe=True, head=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"{file_path2} :\")\n",
    "datasprint(df_train, describe=True, head=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Étude des données\n",
    "\n",
    "> Regarder la donnée en fonction de la résolution\n",
    "\n",
    "Nous allons regarder les données en fonction de la résolution pour voir si nous pouvons trouver des corrélations. Nous allons afficher des histogrammes pour chaque colonne de données en fonction de la résolution. Cela nous permettra de voir si certaines colonnes ont un impact sur la résolution.\n",
    "\n",
    "Nous avons donc constater le nombre de résolutions par crime et très élévé. Nous avons donc décidé de regrouper les résolutions en 4 catégories:\n",
    "- Arrestation / Poursuites : ARREST, BOOKED, ARREST, CITED, JUVENILE CITED, JUVENILE BOOKED, PROSECUTED FOR LESSER OFFENSE, PROSECUTED BY OUTSIDE AGENCY\n",
    "- Absence de poursuites / Refus / Cas particuliers : COMPLAINANT REFUSES TO PROSECUTE, DISTRICT ATTORNEY REFUSES TO PROSECUTE, NOT PROSECUTED, JUVENILE ADMONISHED, JUVENILE DIVERTED, CLEARED-CONTACT JUVENILE FOR MORE INFO, PSYCHOPATHIC CASE, EXCEPTIONAL CLEARANCE\n",
    "- Localisation / État de la personne : LOCATED, UNFOUNDED\n",
    "- Indépendant (NONE) : NONE"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train_columns: list = df_train.columns.tolist()\n",
    "df_train_columns.remove('Category')\n",
    "df_train_columns.remove('Descript')\n",
    "df_train_columns.remove('Resolution')\n",
    "\n",
    "df_train_columns.remove('Address') # trop disparate pour être utile\n",
    "\n",
    "for column in df_train_columns:\n",
    "    graphhistogram(df_train, column, 'Resolution').show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Preprocessing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Categorize 'Resolution' into broader categories\n",
    "df_train['Categorie'] = ''\n",
    "df_train.loc[df_train['Resolution'].isin(['ARREST, BOOKED', 'ARREST, CITED', 'JUVENILE CITED', 'JUVENILE BOOKED', 'PROSECUTED FOR LESSER OFFENSE', 'PROSECUTED BY OUTSIDE AGENCY']), 'Categorie'] = 'Arrestation / Poursuites'\n",
    "df_train.loc[df_train['Resolution'].isin(['COMPLAINANT REFUSES TO PROSECUTE', 'DISTRICT ATTORNEY REFUSES TO PROSECUTE', 'NOT PROSECUTED', 'JUVENILE ADMONISHED', 'JUVENILE DIVERTED', 'CLEARED-CONTACT JUVENILE FOR MORE INFO', 'PSYCHOPATHIC CASE', 'EXCEPTIONAL CLEARANCE']), 'Categorie'] = 'Absence de poursuites / Refus / Cas particuliers'\n",
    "df_train.loc[df_train['Resolution'].isin(['LOCATED', 'UNFOUNDED']), 'Categorie'] = 'Localisation / État de la personne'\n",
    "df_train.loc[df_train['Resolution'].isin(['NONE']), 'Categorie'] = 'Indépendant (NONE)'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Visualization\n",
    "\n",
    "> Nous allons afficher les histogrammes pour chaque colonne de données en fonction de la résolution pour visualiser l'impacte de notre choix de catégorisation."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train_columns: list = df_train.columns.tolist()\n",
    "df_train_columns.remove('Category')\n",
    "df_train_columns.remove('Descript')\n",
    "df_train_columns.remove('Resolution')\n",
    "df_train_columns.remove('Categorie')\n",
    "\n",
    "df_train_columns.remove('Address') # trop disparate pour être utile\n",
    "\n",
    "for column in df_train_columns:\n",
    "    graphhistogram(df_train, column, 'Categorie').show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "graphhistogram(df_train, 'Categorie', 'Resolution').show()"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Feature Selection and Encoding\n",
    "\n",
    "> Nous allons sélectionner les colonnes qui nous intéressent pour l'entraînement du modèle. Nous allons également encoder les colonnes catégorielles en valeurs numériques."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train_patch = df_train[['Dates', 'Categorie', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y']]\n",
    "df_test_patch = df_test[['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y']]\n",
    "\n",
    "categorical_features = [col for col in df_train_patch.columns if df_train[col].dtype == 'object']\n",
    "encoder = OrdinalEncoder(cols=categorical_features).fit(df_train_patch)\n",
    "df_train_patch_2 = encoder.transform(df_train_patch)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Sampling and Splitting Data\n",
    "\n",
    "> Nous allons échantillonner les données pour réduire la taille de l'ensemble d'entraînement. Nous allons également diviser les données en ensembles d'entraînement et de test.\n",
    "\n",
    "Nous avons décidé de prendre 25000 échantillons pour chaque catégorie. Le choix de faire un échantillonage est dû à la taille des données qui est très grande. Nous avons donc décidé de prendre un échantillon pour chaque catégorie pour réduire la taille des données. 25000 est un nombre suffisant pour avoir une bonne précision."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nb = 25000\n",
    "df_train_patch_sample = pd.concat([\n",
    "    df_train_patch_2[df_train_patch_2.Categorie == i].sample(min(nb, len(df_train_patch_2[df_train_patch_2.Categorie == i])), replace=True) \n",
    "    for i in range(4)\n",
    "])\n",
    "\n",
    "y = df_train_patch_sample.Categorie\n",
    "X = df_train_patch_sample.drop(['Categorie'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "> Nous allons entraîner des modèles de classification sur les données d'entraînement et évaluer leur précision sur les données de test.\n",
    "\n",
    "Nous allons entraîner trois modèles de classification différents : Decision Tree, Random Forest et K-Nearest Neighbors.\n",
    "\n",
    "Le choix d'utiliser trois modèles différents est dû à la nature des données. Nous avons des données catégorielles et numériques. Nous avons donc décidé d'utiliser des modèles différents pour voir lequel est le plus performant."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Decision Tree\n",
    "clf = tree.DecisionTreeClassifier().fit(X_train, y_train)\n",
    "acctree = clf.score(X_test, y_test) * 100\n",
    "print(f'Decision Tree Accuracy: {acctree:.2f}%')\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n",
    "accrf = accuracy_score(y_test, rf_classifier.predict(X_test)) * 100\n",
    "print(f'Random Forest Accuracy: {accrf:.2f}%')\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=2).fit(X_train, y_train)\n",
    "accknn = accuracy_score(y_test, knn.predict(X_test)) * 100\n",
    "print(f'KNN Accuracy: {accknn:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## KNeighborsClassifier - Choosing the Best K Value\n",
    "\n",
    "> Nous allons choisir la meilleure valeur de k pour le modèle K-Nearest Neighbors en traçant l'exactitude en fonction de différentes valeurs de k.\n",
    "\n",
    "Nous avons décidé de choisir la valeur de k qui donne la meilleure précision pour le modèle K-Nearest Neighbors.\n",
    "\n",
    "On remarque que la précision est maximale pour k=2."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k_values = range(1, 21)\n",
    "accuracies = []\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "plt.plot(k_values, accuracies)\n",
    "plt.xlabel('Number of Neighbors (k)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy for different k values')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Overall Accuracy"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Overall Accuracy\")\n",
    "print(f'Decision Tree: {acctree:.2f}%')\n",
    "print(f'Random Forest: {accrf:.2f}%')\n",
    "print(f'KNN: {accknn:.2f}%')\n",
    "glbacc = (acctree + accrf + accknn) / 3\n",
    "print(f'Global: {glbacc:.2f}%')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predict New Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*Downcasting object dtype arrays on .fillna.*\")\n",
    "\n",
    "new_data = {\n",
    "    \"Dates\": [\"2015-05-10 23:59:00\"],\n",
    "    \"DayOfWeek\": [\"Sunday\"],\n",
    "    \"PdDistrict\": [\"BAYVIEW\"],\n",
    "    \"Address\": [\"2000 Block of THOMAS AV\"],\n",
    "    \"X\": [-122.39958770418998],\n",
    "    \"Y\": [37.7350510103906],\n",
    "    \"Categorie\": [None]\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "new_df_encoded = encoder.transform(new_df).drop(columns=['Categorie'])\n",
    "\n",
    "tree_prediction = clf.predict(new_df_encoded)\n",
    "rf_prediction = rf_classifier.predict(new_df_encoded)\n",
    "knn_prediction = knn.predict(new_df_encoded)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final Prediction Using Majority Vote"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = [tuple(tree_prediction), tuple(rf_prediction), tuple(knn_prediction)]\n",
    "prediction_counts = Counter(predictions)\n",
    "most_common_prediction, count = prediction_counts.most_common(1)[0]\n",
    "\n",
    "if count == 1:\n",
    "    accuracies = {'tree': acctree, 'rf': accrf, 'knn': accknn}\n",
    "    most_accurate_model = max(accuracies, key=accuracies.get)\n",
    "    final_prediction = eval(f'{most_accurate_model}_prediction')\n",
    "else:\n",
    "    final_prediction = most_common_prediction\n",
    "\n",
    "final_prediction = int(final_prediction[0])\n",
    "print(f\"Final Prediction: {final_prediction}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Map Prediction to Category"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prediction_mapping = {\n",
    "    0: 'Arrestation / Poursuites',\n",
    "    1: 'Absence de poursuites / Refus / Cas particuliers',\n",
    "    2: 'Localisation / État de la personne',\n",
    "    3: 'Indépendant (NONE)'\n",
    "}\n",
    "\n",
    "final_prediction_text = prediction_mapping.get(final_prediction, \"Catégorie inconnue\")\n",
    "print(f\"Final Prediction: {final_prediction_text}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Aprés avoir fait l'analyse des données\n",
    "\n",
    "> Non implémenté\n",
    "\n",
    "Nous avons décidé de spliter la date dans le but de préciser les décisions prises en fonction des Date ou de l'horraire\n",
    "- création de la colonne 'Date'\n",
    "- création de la colonne 'Heure'\n",
    "- création de la colonne 'MonthofYear'"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_train['Date'] = pd.to_datetime(df_train['Dates']).dt.date\n",
    "df_train['Heure'] = pd.to_datetime(df_train['Dates']).dt.time\n",
    "df_train['MonthofYear'] = pd.to_datetime(df_train['Date']).dt.month\n",
    "\n",
    "# print(df_train)\n",
    "\n",
    "df_test['Date'] = pd.to_datetime(df_test['Dates']).dt.date\n",
    "df_test['Heure'] = pd.to_datetime(df_test['Dates']).dt.time\n",
    "df_test['MonthofYear'] = pd.to_datetime(df_test['Date']).dt.month\n",
    "\n",
    "# print(df_test)\n",
    "\n",
    "df_train_patch = df_train[['Dates', 'Categorie', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'Date', 'Heure', 'MonthofYear']]\n",
    "df_test_patch = df_test[['Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y', 'Date', 'Heure', 'MonthofYear']]\n",
    "\n",
    "categorical_features = [col for col in df_train_patch.columns if df_train[col].dtype == 'object']\n",
    "encoder = OrdinalEncoder(cols=categorical_features).fit(df_train_patch)\n",
    "df_train_patch_2 = encoder.transform(df_train_patch)\n",
    "\n",
    "nb = 25000\n",
    "df_train_patch_sample = pd.concat([\n",
    "    df_train_patch_2[df_train_patch_2.Categorie == i].sample(min(nb, len(df_train_patch_2[df_train_patch_2.Categorie == i])), replace=True)\n",
    "    for i in range(4)\n",
    "])\n",
    "\n",
    "y = df_train_patch_sample.Categorie\n",
    "X = df_train_patch_sample.drop(['Categorie'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Decision Tree\n",
    "clf = tree.DecisionTreeClassifier().fit(X_train, y_train)\n",
    "acctree = clf.score(X_test, y_test) * 100\n",
    "print(f'Decision Tree Accuracy: {acctree:.2f}%')\n",
    "\n",
    "# Random Forest\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42).fit(X_train, y_train)\n",
    "accrf = accuracy_score(y_test, rf_classifier.predict(X_test)) * 100\n",
    "print(f'Random Forest Accuracy: {accrf:.2f}%')\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=2).fit(X_train, y_train)\n",
    "accknn = accuracy_score(y_test, knn.predict(X_test)) * 100\n",
    "print(f'KNN Accuracy: {accknn:.2f}%')\n",
    "\n",
    "print(\"Overall Accuracy\")\n",
    "print(f'Decision Tree: {acctree:.2f}%')\n",
    "print(f'Random Forest: {accrf:.2f}%')\n",
    "print(f'KNN: {accknn:.2f}%')\n",
    "glbacc = (acctree + accrf + accknn) / 3\n",
    "print(f'Global: {glbacc:.2f}%')\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*Downcasting object dtype arrays on .fillna.*\")\n",
    "\n",
    "new_data = {\n",
    "    \"Dates\": [\"2015-05-10 23:59:00\"],\n",
    "    \"DayOfWeek\": [\"Sunday\"],\n",
    "    \"PdDistrict\": [\"BAYVIEW\"],\n",
    "    \"Address\": [\"2000 Block of THOMAS AV\"],\n",
    "    \"X\": [-122.39958770418998],\n",
    "    \"Y\": [37.7350510103906],\n",
    "    \"Categorie\": [None],\n",
    "    \"Date\": [pd.to_datetime(\"2015-05-10 23:59:00\").date()],\n",
    "    \"Heure\": [pd.to_datetime(\"2015-05-10 23:59:00\").time()],\n",
    "    \"MonthofYear\": [pd.to_datetime(\"2015-05-10 23:59:00\").month]\n",
    "}\n",
    "\n",
    "new_df = pd.DataFrame(new_data)\n",
    "new_df_encoded = encoder.transform(new_df).drop(columns=['Categorie'])\n",
    "\n",
    "tree_prediction = clf.predict(new_df_encoded)\n",
    "rf_prediction = rf_classifier.predict(new_df_encoded)\n",
    "knn_prediction = knn.predict(new_df_encoded)\n",
    "\n",
    "predictions = [tuple(tree_prediction), tuple(rf_prediction), tuple(knn_prediction)]\n",
    "prediction_counts = Counter(predictions)\n",
    "most_common_prediction, count = prediction_counts.most_common(1)[0]\n",
    "\n",
    "if count == 1:\n",
    "    accuracies = {'tree': acctree, 'rf': accrf, 'knn': accknn}\n",
    "    most_accurate_model = max(accuracies, key=accuracies.get)\n",
    "    final_prediction = eval(f'{most_accurate_model}_prediction')\n",
    "else:\n",
    "    final_prediction = most_common_prediction\n",
    "\n",
    "final_prediction = int(final_prediction[0])\n",
    "print(f\"Final Prediction: {final_prediction}\")\n",
    "\n",
    "prediction_mapping = {\n",
    "    0: 'Arrestation / Poursuites',\n",
    "    1: 'Absence de poursuites / Refus / Cas particuliers',\n",
    "    2: 'Localisation / État de la personne',\n",
    "    3: 'Indépendant (NONE)'\n",
    "}\n",
    "\n",
    "final_prediction_text = prediction_mapping.get(final_prediction, \"Catégorie inconnue\")\n",
    "print(f\"Final Prediction: {final_prediction_text}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}
